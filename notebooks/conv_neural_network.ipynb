{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b18a8b02-d963-490b-ad65-084cd09708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset loading\n",
    "from typing import Tuple\n",
    "from pickle import load, dump\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from src.vg2signal import read_raw_vg_as_df, make_smoother, make_shoulder_getter, make_detilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3da3eaad-321f-462a-960d-6e391b00d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convVolta(nn.Module):\n",
    "    def __init__(self, input_dim,\n",
    "                       input_channel,\n",
    "                       kernel_size):\n",
    "        super(convVolta, self).__init__()\n",
    "        self.conv1   = nn.Conv1d(1,  input_channel, kernel_size,     stride=2)\n",
    "        self.conv2   = nn.Conv1d(input_channel, input_channel//2, kernel_size, stride=2)\n",
    "        self.conv3   = nn.Conv1d(input_channel//2, input_channel//4, kernel_size, stride=2)\n",
    "        self.conv4   = nn.Conv1d(input_channel//4, 1, kernel_size, stride=2)\n",
    "        \n",
    "        # self.final   = nn.Linear(input_dim, 1)\n",
    "        self.pool    = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4e0d268-b8f3-4ee2-ae3f-01cec2b739ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2signal_extra_features(vg_filename: str,\n",
    "             do_log: bool,\n",
    "             smoothing_bw: float,\n",
    "             vcenter: float,\n",
    "             vwidth: float,\n",
    "             stiffness: float):\n",
    "\n",
    "    vg_df = read_raw_vg_as_df(vg_filename)\n",
    "\n",
    "    if (vg_df['I'].to_numpy() < 0).any():\n",
    "        temp = [None] * 11\n",
    "        return [None, None, vg_df] + temp\n",
    "\n",
    "    if do_log:\n",
    "        cur_var_name = \"logI\"\n",
    "        #vg_df[cur_var_name] = np.emath.logn(logbase, vg_df[\"I\"])\n",
    "        vg_df[cur_var_name] = np.log2(vg_df[\"I\"])\n",
    "    else:\n",
    "        cur_var_name = \"I\"\n",
    "\n",
    "    smoother = make_smoother(smoothing_bw)\n",
    "\n",
    "    vg_df[\"smoothed\"] = smoother(vg_df[\"V\"], vg_df[cur_var_name].to_numpy())\n",
    "\n",
    "    shoulder_getter = make_shoulder_getter(1, 1.1)\n",
    "    (peak_signal, peak_v_shoulder) = shoulder_getter(vg_df[\"V\"],\n",
    "                                                     vg_df[\"smoothed\"])\n",
    "\n",
    "    vcenter = peak_v_shoulder\n",
    "    vstart = vcenter - 0.5*vwidth\n",
    "    vend = vcenter + 0.5*vwidth\n",
    "\n",
    "    detilter = make_detilter(vstart, vend, stiffness)\n",
    "    vg_df[\"detilted\"] = detilter(vg_df[\"V\"].to_numpy(),\n",
    "                                 vg_df[\"smoothed\"].to_numpy())\n",
    "\n",
    "   \n",
    "    return  vg_df\n",
    "\n",
    "def run_vg2_raw(folderpath: str, \n",
    "            do_log:bool, \n",
    "            recenter:bool, \n",
    "            smoothing_bw:float, \n",
    "            stiffness:float, \n",
    "            vcenter:float, \n",
    "            vwidth1:float, \n",
    "            vwidth2:float) -> Tuple[dict, str]:\n",
    "\n",
    "    os.chdir(folderpath)  # change to desired folderpath\n",
    "    dfxl     = pd.DataFrame(columns=['labels', 'VI'])\n",
    "    \n",
    "    for filename in os.listdir():\n",
    "        if filename[-3:] == 'txt':\n",
    "            print(\"Analyzing:\", filename)\n",
    "            df = v2signal_extra_features(filename,\n",
    "                                        do_log,\n",
    "                                        smoothing_bw,\n",
    "                                        vcenter,\n",
    "                                        vwidth1,\n",
    "                                        stiffness)\n",
    "\n",
    "            idx1 = filename.rfind(\"cbz\")\n",
    "            idx2 = filename[idx1:].find(\"_\")\n",
    "            conc = filename[idx1 + 3:idx1 + idx2]\n",
    "            replicate = filename[idx1 + idx2 + 1:filename.rfind(\".\")]\n",
    "\n",
    "            # Crop only the signal\n",
    "            try:\n",
    "                crop = (df['V']<1.15) & (df['V']>0.93)\n",
    "                VI   =  df['detilted'][crop].to_numpy() \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if 'p' in conc:  \n",
    "                pi = conc.find('p')\n",
    "                conctemp = conc[:pi] + '.' + conc[pi + 1:]\n",
    "                conc = conctemp\n",
    "            \n",
    "            concstrxl   = str(float(conc))\n",
    "            dfxl = pd.concat([dfxl, pd.DataFrame({'labels':concstrxl, 'VI':[VI]})])\n",
    "\n",
    "    with open(f\"{folderpath}/raw_data.pickle\", 'wb') as f:\n",
    "        dump(dfxl, f)\n",
    "   \n",
    "    return    dfxl  \n",
    "\n",
    "def load_dataset(dataset_path=None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    if dataset_path==None: dataset_path = DATASET_PATH\n",
    "\n",
    "    if ('ML1_ML2'in os.path.basename(dataset_path)) or ('test' in os.path.basename(dataset_path)):\n",
    "        datasets = sorted([f\"{i}/raw_data.pickle\" for i in glob(f'{dataset_path}/*')])\n",
    "        \n",
    "        df = []\n",
    "        for dataset in datasets:\n",
    "\n",
    "            with open(dataset, 'rb') as f:\n",
    "                data = load(f)\n",
    "                df.append(data) \n",
    "        df = pd.concat(df)\n",
    "\n",
    "    else:\n",
    "        dataset_path = f\"{dataset_path}/raw_data.pickle\"\n",
    "        with open(dataset_path, 'rb') as f:\n",
    "            df = load(f)\n",
    "\n",
    "    X = [x.iloc[0] for (_,x) in df[['VI']].iterrows()] \n",
    "    X = np.array(X)\n",
    "    y = df['labels'].apply(lambda x: float(x))\n",
    "\n",
    "    # Split the total dataset into training (60%) and testing (40%) dataset\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=20, stratify=y)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f90cf61-c596-44d1-b30b-b1b611a2ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of parameters:\n",
    "input_dim     = 55\n",
    "input_channel = 64\n",
    "kernel_size   = 3\n",
    "\n",
    "model       = convVolta(input_dim, input_channel, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca8e3fdd-84e5-4110-bb8e-1bf5c37ece51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8033"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = np.sum([np.prod(parameter.detach().numpy().shape) for parameter in model.parameters() if parameter.requires_grad])\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48eb5f61-720c-4108-9152-b2a444cee2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test), _ =  load_dataset('/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f480ef8f-0c69-40bd-adbb-d5bd11a7c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), _  = load_dataset('/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_19_ML1')\n",
    "(ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), _  = load_dataset('/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_22_ML2')\n",
    "(ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), _  = load_dataset('/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f385a2f5-2cfc-43a2-9845-4395880d10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([ML1_X_train, ML2_X_train, ML4_X_train], axis=0)\n",
    "X_test  = np.concatenate([ML1_X_test, ML2_X_test, ML4_X_test], axis=0)\n",
    "y_train = np.concatenate([ML1_y_train, ML2_y_train, ML4_y_train], axis=0)\n",
    "y_test  = np.concatenate([ML1_y_test, ML2_y_test, ML4_y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54c12d0e-d1aa-4a6c-9214-028d6434a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangam/miniconda3/envs/vgramreg/lib/python3.9/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:0.19\n",
      "Epoch:500 | Loss:0.14\n",
      "Epoch:1000 | Loss:0.18\n",
      "Epoch:1500 | Loss:0.16\n",
      "Epoch:2000 | Loss:0.17\n",
      "Epoch:2500 | Loss:0.16\n",
      "Epoch:3000 | Loss:0.20\n",
      "Epoch:3500 | Loss:0.17\n",
      "Epoch:4000 | Loss:0.16\n",
      "Epoch:4500 | Loss:0.17\n",
      "Epoch:5000 | Loss:0.15\n",
      "Epoch:5500 | Loss:0.14\n",
      "Epoch:6000 | Loss:0.16\n",
      "Epoch:6500 | Loss:0.17\n",
      "Epoch:7000 | Loss:0.15\n",
      "Epoch:7500 | Loss:0.16\n",
      "Epoch:8000 | Loss:0.19\n",
      "Epoch:8500 | Loss:0.17\n",
      "Epoch:9000 | Loss:0.18\n",
      "Epoch:9500 | Loss:0.18\n",
      "Epoch:10000 | Loss:0.18\n",
      "Epoch:10500 | Loss:0.18\n",
      "Epoch:11000 | Loss:0.16\n",
      "Epoch:11500 | Loss:0.16\n",
      "Epoch:12000 | Loss:0.19\n",
      "Epoch:12500 | Loss:0.16\n",
      "Epoch:13000 | Loss:0.14\n",
      "Epoch:13500 | Loss:0.19\n",
      "Epoch:14000 | Loss:0.18\n",
      "Epoch:14500 | Loss:0.16\n",
      "Epoch:15000 | Loss:0.16\n",
      "Epoch:15500 | Loss:0.16\n",
      "Epoch:16000 | Loss:0.15\n",
      "Epoch:16500 | Loss:0.19\n",
      "Epoch:17000 | Loss:0.13\n",
      "Epoch:17500 | Loss:0.20\n",
      "Epoch:18000 | Loss:0.19\n",
      "Epoch:18500 | Loss:0.16\n",
      "Epoch:19000 | Loss:0.19\n",
      "Epoch:19500 | Loss:0.19\n",
      "Epoch:20000 | Loss:0.14\n",
      "Epoch:20500 | Loss:0.18\n",
      "Epoch:21000 | Loss:0.17\n",
      "Epoch:21500 | Loss:0.19\n",
      "Epoch:22000 | Loss:0.19\n",
      "Epoch:22500 | Loss:0.20\n",
      "Epoch:23000 | Loss:0.16\n",
      "Epoch:23500 | Loss:0.16\n",
      "Epoch:24000 | Loss:0.17\n",
      "Epoch:24500 | Loss:0.16\n",
      "Epoch:25000 | Loss:0.17\n",
      "Epoch:25500 | Loss:0.18\n",
      "Epoch:26000 | Loss:0.16\n",
      "Epoch:26500 | Loss:0.17\n",
      "Epoch:27000 | Loss:0.19\n",
      "Epoch:27500 | Loss:0.13\n",
      "Epoch:28000 | Loss:0.15\n",
      "Epoch:28500 | Loss:0.19\n",
      "Epoch:29000 | Loss:0.18\n",
      "Epoch:29500 | Loss:0.19\n",
      "Epoch:30000 | Loss:0.17\n",
      "Epoch:30500 | Loss:0.17\n",
      "Epoch:31000 | Loss:0.16\n",
      "Epoch:31500 | Loss:0.19\n",
      "Epoch:32000 | Loss:0.19\n",
      "Epoch:32500 | Loss:0.17\n",
      "Epoch:33000 | Loss:0.16\n",
      "Epoch:33500 | Loss:0.18\n",
      "Epoch:34000 | Loss:0.15\n",
      "Epoch:34500 | Loss:0.15\n",
      "Epoch:35000 | Loss:0.16\n",
      "Epoch:35500 | Loss:0.16\n",
      "Epoch:36000 | Loss:0.16\n",
      "Epoch:36500 | Loss:0.16\n",
      "Epoch:37000 | Loss:0.16\n",
      "Epoch:37500 | Loss:0.15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criteron(output, y_)\n\u001b[1;32m     31\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/vgramreg/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vgramreg/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vgramreg/lib/python3.9/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "iteration    = 100000\n",
    "lr           = 1e-5\n",
    "batch_size   = 32\n",
    "num_datasize = len(X_train)\n",
    "\n",
    "# Define loss function\n",
    "criteron  = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optim     = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "num_step  = num_datasize // batch_size\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    model.train()\n",
    "    ind_shuffle = np.random.permutation(num_datasize)\n",
    "    X_train        = X_train[ind_shuffle]\n",
    "    y_train        = y_train[ind_shuffle]\n",
    "\n",
    "    for i in range(num_step):\n",
    "        start = i*batch_size\n",
    "        end   = start + batch_size\n",
    "\n",
    "        X_    = torch.tensor(np.transpose(X_train[start:end][np.newaxis,...], (1,0,2)), dtype=torch.float32)\n",
    "        y_    = torch.tensor(y_train[start:end]/16.0, dtype=torch.float32)\n",
    "        \n",
    "        # Train Deep Learning Model\n",
    "        output = model(X_)\n",
    "        loss   = criteron(output, y_)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    if (epoch%500 == 0):\n",
    "        print(f\"Epoch:{epoch} | Loss:{loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e41d0fb9-5262-4020-b477-54e38cae1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t       = torch.tensor(np.transpose(X_test[np.newaxis,...], (1,0,2)), dtype=torch.float32)\n",
    "test_pred = model(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8aa0ed2c-5f0f-42e6-9f39-ad5d9aa35d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02945621556021727"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, test_pred.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a611eaf-874f-4dee-8cf3-d0963925f97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.090213 , 7.721645 , 8.065046 , 8.034036 , 7.9796767, 7.71783  ,\n",
       "       7.78349  , 8.050802 , 7.7186203, 7.7165174, 7.8573875, 7.936646 ,\n",
       "       8.125941 , 7.9429097, 8.038576 , 7.728791 , 7.989955 , 8.0466795,\n",
       "       7.9552026, 7.7020636, 7.9745417, 8.067125 , 7.7954593, 7.7102504,\n",
       "       7.716678 , 7.738204 , 8.108349 , 8.061393 , 7.887176 , 8.079269 ,\n",
       "       8.05351  , 7.724674 , 7.7506037, 8.13549  , 7.715234 , 7.9610424,\n",
       "       8.068936 , 7.717053 , 7.710494 , 7.738425 , 8.000651 , 8.063717 ,\n",
       "       7.7187304, 7.985941 , 8.155564 , 8.053347 , 7.7255297, 8.021147 ,\n",
       "       8.143539 , 7.7249837, 8.101517 , 8.023556 , 8.11741  , 7.730401 ,\n",
       "       8.133028 , 7.8790226, 8.098868 , 7.7217765, 7.7202053, 8.086627 ,\n",
       "       7.894541 , 8.076915 , 7.8489404, 7.9665785, 7.7393403, 7.913589 ,\n",
       "       7.7058344, 7.9713373, 7.7234445, 7.963597 , 8.13247  , 8.056703 ,\n",
       "       8.134447 , 7.942874 , 8.009087 , 7.800006 , 7.745957 , 7.705672 ,\n",
       "       7.8689437, 7.9988728, 7.71984  , 7.722468 , 7.850614 , 7.70311  ,\n",
       "       7.927786 , 8.124931 , 8.066276 , 7.7330093, 7.9137197, 7.7202063,\n",
       "       7.983146 , 7.895037 , 8.067759 , 7.7362432, 7.7572308, 8.147256 ,\n",
       "       8.044418 , 8.027319 , 7.711902 , 8.076608 , 7.7106314, 7.983674 ,\n",
       "       7.961403 , 8.09977  , 7.705976 , 7.7463646, 8.124008 , 8.090891 ,\n",
       "       8.053247 , 8.09856  , 8.206696 , 7.712466 , 8.13669  , 7.7199574,\n",
       "       8.111661 , 7.714651 , 8.133594 , 7.92846  , 8.149751 , 8.1119995,\n",
       "       8.103248 , 7.9569173, 7.7162027, 7.7051463, 8.140797 , 8.194935 ,\n",
       "       7.722523 , 7.9669757, 7.9458323, 7.712996 , 8.037432 , 8.13233  ,\n",
       "       7.9644322, 7.701056 , 8.119825 , 7.7241993, 8.143148 , 8.048982 ,\n",
       "       8.209163 , 7.7164407, 7.7157326, 8.046271 , 8.051142 , 7.9577723,\n",
       "       7.700178 , 8.119822 ], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f31db8-662e-44cf-a95d-d24c0ea34006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
