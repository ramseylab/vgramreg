{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f4ee59-02d0-4aa3-90f8-0384d93204fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from src.load_models import select_model\n",
    "from src.load_dataset import load_dataset\n",
    "from src.config import models_features_r2, models_features_per, model_name_conversion\n",
    "from src.graph_visualization import visualize_highest_score_feature_selection, feature_selection_tabularize, visualization_testing_dataset\n",
    "\n",
    "from src.utils import find_adj_score, calculate_y_LOD, per_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42565fd1-4a4e-4da9-b062-f1a5762f8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection():\n",
    "    def __init__(self, model_name:str, X_train:pd.DataFrame, y_train:pd.Series):\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.model = select_model(model_name)\n",
    "        self.all_feature_scores = []\n",
    "    \n",
    "        self.y_LOD = calculate_y_LOD(self.X_train, self.y_train) \n",
    "\n",
    "    def save(self, path:str) -> None:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.model, path) \n",
    "\n",
    "    def find_score(self, kf:KFold, features:list) -> np.ndarray:\n",
    "        return np.array(self.calculate_r2_score(self.model, self.X_train[features], self.y_train, kf))\n",
    "    \n",
    "    def find_per_diff(self, kf:KFold, features:list) -> np.ndarray:\n",
    "        return np.array(self.calculate_per_diff(self.model, self.X_train[features], self.y_train, kf))\n",
    "    \n",
    "    def calculate_per_diff(self, model:BaseEstimator, X:pd.DataFrame, y:pd.Series, kf:KFold) -> np.ndarray:\n",
    "        per_diff_all = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            model_ = clone(model)\n",
    "            \n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
    "        \n",
    "            model_.fit(X_train, y_train)\n",
    "            \n",
    "            mask           = (y_test != 0)    # Non Zero Concentration\n",
    "            zero_mask      = ~(mask)          # Zero Concentration\n",
    "\n",
    "            y_pred         = model_.predict(X_test)\n",
    "            y_pred         = np.maximum(y_pred, 0.0)\n",
    "\n",
    "            # Only for non zero concentration\n",
    "            non_zero_per_error = np.abs(y_test[mask] - y_pred[mask])/(0.5*(y_test[mask] + y_pred[mask]))\n",
    "           \n",
    "            # zero concentration\n",
    "            zero_per_error     = np.abs(y_test[zero_mask] - y_pred[zero_mask]) / self.y_LOD\n",
    "\n",
    "            assert not(np.isnan(zero_per_error).any())\n",
    "            assert not(np.isnan(non_zero_per_error).any())\n",
    "\n",
    "            per_error         = np.concatenate((non_zero_per_error, zero_per_error))\n",
    "            per_error         = np.mean(per_error) * 100\n",
    "\n",
    "            assert not(np.isnan(per_error)) # To check if any output is invalid or nan\n",
    "            per_diff_all.append(per_error)\n",
    "\n",
    "        \n",
    "        return np.array(per_diff_all).mean()\n",
    "    \n",
    "\n",
    "    def calculate_r2_score(self, model:BaseEstimator, X:pd.DataFrame, y:pd.Series, kf:KFold) -> np.ndarray:\n",
    "        scores, adj_scores = [], []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            model_ = clone(model)\n",
    "            \n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
    "        \n",
    "            model_.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred         = model_.predict(X_test)\n",
    "            y_pred         = np.maximum(y_pred, 0.0)\n",
    "\n",
    "            score          = r2_score(y_test, y_pred)\n",
    "\n",
    "            adj_score      = find_adj_score(len(y_pred), X_train.shape[1], score) # N, P, R2 score\n",
    "\n",
    "            scores.append(score)\n",
    "            adj_scores.append(adj_score)\n",
    "\n",
    "        return np.array(scores).mean(), np.array(adj_scores).mean()\n",
    "    \n",
    "    def fit(self, features:list) -> None:\n",
    "        self.model.fit(self.X_train[features], self.y_train)\n",
    "\n",
    "    def find_best_features(self, kf:KFold, r2_score:float) -> list:\n",
    "        model = clone(self.model)\n",
    "\n",
    "        all_features            = self.X_train.columns.values\n",
    "        self.selected_features  = all_features.copy().tolist()\n",
    "        self.all_feature_scores = []\n",
    "\n",
    "        best_score        = self.calculate_r2_score(model, self.X_train[all_features], self.y_train, kf) if r2_score else \\\n",
    "                            self.calculate_per_diff(model, self.X_train[all_features], self.y_train, kf)\n",
    "        \n",
    "        flag              = False\n",
    "\n",
    "        while len(self.selected_features) != 0:\n",
    "            one_line_score    = []\n",
    "            one_line_features = []\n",
    "            \n",
    "            for feature in all_features:\n",
    "                if feature in self.selected_features: \n",
    "                    testing_feature = [i for i in self.selected_features if i != feature] # Remove the feature from the set\n",
    "                    \n",
    "                    if r2_score:\n",
    "                        score = self.calculate_r2_score(model, self.X_train[testing_feature], self.y_train, kf)\n",
    "                    else:  \n",
    "                        score = self.calculate_per_diff(model, self.X_train[testing_feature], self.y_train, kf)\n",
    "                    \n",
    "                    one_line_score.append(score)\n",
    "                    one_line_features.append(feature)\n",
    "           \n",
    "            one_line_score = np.array(one_line_score) if r2_score else one_line_score\n",
    "            \n",
    "            if r2_score==True:\n",
    "                best_socre_ind      = np.argmax(one_line_score[:,0])\n",
    "                one_line_best_score = one_line_score[best_socre_ind]\n",
    "\n",
    "            else:\n",
    "                best_socre_ind, one_line_best_score = np.argmin(one_line_score), np.min(one_line_score)\n",
    "\n",
    "            sel_one_line_feature    = one_line_features[best_socre_ind] \n",
    "\n",
    "            temp = {}\n",
    "            for key, score in zip(one_line_features, one_line_score):\n",
    "                # key = [i for i in self.selected_features if i != key]\n",
    "                temp[str(key)] = score\n",
    "\n",
    "            if r2_score:\n",
    "                if one_line_best_score[0] > best_score[0]:\n",
    "                    best_score = one_line_best_score\n",
    "                    self.selected_features.remove(sel_one_line_feature)\n",
    "                    self.all_feature_scores.append(temp)\n",
    "                    flag = False\n",
    "\n",
    "                else: flag = True\n",
    "                        \n",
    "            else:\n",
    "                if one_line_best_score <= best_score:\n",
    "                    best_score = one_line_best_score\n",
    "                    self.selected_features.remove(sel_one_line_feature)\n",
    "                    self.all_feature_scores.append(temp)\n",
    "                    flag = False\n",
    "\n",
    "                else: flag = True\n",
    "\n",
    "            if flag: break\n",
    "        \n",
    "        self.best_score = best_score\n",
    "        if self.all_feature_scores == []: self.all_feature_scores.append({str(self.selected_features): best_score[0]})\n",
    "        return self.all_feature_scores\n",
    "    \n",
    "    def find_testing_score(self, X_test: pd.DataFrame, y_test: pd.DataFrame) -> Tuple[list, list]:\n",
    "\n",
    "        # Fit the model with selected features\n",
    "        self.model.fit(self.X_train[self.selected_features], self.y_train)\n",
    "\n",
    "        # Return both training and testing r2 score\n",
    "        return self.model.score(self.X_train[self.selected_features], self.y_train), \\\n",
    "               self.model.score(X_test[self.selected_features], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d96031a-4abe-4629-a959-3f4605a20f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_performance_metric(model_names: list, r2_top:pd.DataFrame) -> Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "        Calcualte R2 Score and Percent Error on testing dataset\n",
    "    \"\"\"\n",
    "\n",
    "    r2_scores  = {'Models':[], 'Scores':[]}\n",
    "    per_errors = {'Models':[], 'Scores':[]}\n",
    "\n",
    "    model_names = model_names if not(only_one_multivariate) else r2_top['Models'].values.tolist()\n",
    "\n",
    "    print(model_names)\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        model_name = 'Linear' if ((model_name == 'multivariate')) else model_name\n",
    "\n",
    "        model_r2   = ModelSelection(model_name, X_train, y_train)\n",
    "        model_per  = ModelSelection(model_name, X_train, y_train)\n",
    "        \n",
    "        model_r2.fit(models_features_r2[model_name])\n",
    "        model_per.fit(models_features_per[model_name])\n",
    "\n",
    "        y_pred_r2  = model_r2.model.predict(X_test[models_features_r2[model_name]])\n",
    "        y_pred_per = model_per.model.predict(X_test[models_features_per[model_name]])\n",
    "\n",
    "        model_per_error      = per_error(y_test, y_pred_per, model_per.y_LOD)\n",
    "        model_r2_score       = r2_score(y_test, y_pred_r2)\n",
    "        model_r2_adj         = find_adj_score(len(y_test), len(models_features_r2[model_name]), model_r2_score)  # Numer of testing dataset, number of features, R2\n",
    "\n",
    "        model_name = 'multivariate' if ((model_name == 'Linear') and only_one_multivariate) else model_name\n",
    "\n",
    "        r2_scores['Models'].append(model_name) \n",
    "        per_errors['Models'].append(model_name)\n",
    "        \n",
    "        \n",
    "        r2_scores['Scores'].append((model_r2_score, model_r2_adj))\n",
    "        per_errors['Scores'].append(model_per_error)\n",
    "\n",
    "    return r2_scores, per_errors\n",
    "    \n",
    "def select_features(X_train: pd.DataFrame, y_train: pd.DataFrame, model_names: list) -> Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "        This function selects the best feature combinations for both \n",
    "        R2 score and percent error metrics for the given lists of models\n",
    "    \"\"\"\n",
    "\n",
    "    feature_selection_r2score     = {}\n",
    "    feature_selection_per_diff    = {}\n",
    "\n",
    "    kf        = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for model_name in model_names:\n",
    "        dataset_model_name = model_name\n",
    "       \n",
    "        model = ModelSelection(model_name, X_train, y_train)\n",
    "\n",
    "        feature_selection_r2score[dataset_model_name]  = model.find_best_features(kf, r2_score=True)\n",
    "        models_features_r2[model_name] = model.selected_features\n",
    "\n",
    "        feature_selection_per_diff[dataset_model_name] = model.find_best_features(kf, r2_score=False)\n",
    "        models_features_per[model_name] = model.selected_features\n",
    "\n",
    "        print(f\"{model_name} R2 Score Best Feature\",      models_features_r2[model_name])\n",
    "        print(f\"{model_name} Percent Error Best Feature\", models_features_per[model_name])\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "\n",
    "    return feature_selection_r2score, feature_selection_per_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95cd0a9-23ff-4be0-881d-07cb2812ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangam/Desktop/Epilepsey/Code/vgramreg/src/load_dataset.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######Data Distribution:#########\n",
      "Training {0: 23, 8: 25, 16: 21}\n",
      "Testing {0: 15, 8: 16, 16: 15}\n",
      "#################################\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m comparision_model     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muni_multivariate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m only_one_multivariate \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_nonlinear\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m load_dataset()\n\u001b[0;32m---> 10\u001b[0m feature_selection_r2score, feature_selection_per_diff \u001b[38;5;241m=\u001b[39m \u001b[43mselect_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m, in \u001b[0;36mselect_features\u001b[0;34m(X_train, y_train, model_names)\u001b[0m\n\u001b[1;32m     52\u001b[0m dataset_model_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelSelection(model_name, X_train, y_train)\n\u001b[0;32m---> 56\u001b[0m feature_selection_r2score[dataset_model_name]  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr2_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m models_features_r2[model_name] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mselected_features\n\u001b[1;32m     59\u001b[0m feature_selection_per_diff[dataset_model_name] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfind_best_features(kf, r2_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m, in \u001b[0;36mModelSelection.find_best_features\u001b[0;34m(self, kf, r2_score)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_features  \u001b[38;5;241m=\u001b[39m all_features\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_feature_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 90\u001b[0m best_score        \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_r2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m r2_score \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m     91\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_per_diff(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train[all_features], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, kf)\n\u001b[1;32m     93\u001b[0m flag              \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_features) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mModelSelection.calculate_r2_score\u001b[0;34m(self, model, X, y, kf)\u001b[0m\n\u001b[1;32m     69\u001b[0m y_pred         \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(y_pred, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     71\u001b[0m score          \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred)\n\u001b[0;32m---> 73\u001b[0m adj_score      \u001b[38;5;241m=\u001b[39m \u001b[43mfind_adj_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# N, P, R2 score\u001b[39;00m\n\u001b[1;32m     75\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     76\u001b[0m adj_scores\u001b[38;5;241m.\u001b[39mappend(adj_score)\n",
      "File \u001b[0;32m~/Desktop/Epilepsey/Code/vgramreg/src/utils.py:7\u001b[0m, in \u001b[0;36mfind_adj_score\u001b[0;34m(N, P, R_2)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_adj_score\u001b[39m(N: \u001b[38;5;28mint\u001b[39m, P: \u001b[38;5;28mint\u001b[39m, R_2: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR_2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model_names = ['Linear', 'KNN', 'SVM', 'RF', 'GP']\n",
    "OUTPUT_PATH = 'Outputs'\n",
    "\n",
    "only_one_multivariate = False\n",
    "adj_score             = False \n",
    "\n",
    "comparision_model     = 'uni_multivariate' if only_one_multivariate else 'linear_nonlinear'\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_dataset()\n",
    "feature_selection_r2score, feature_selection_per_diff = select_features(X_train, y_train, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cbb32-2f89-4d44-a0a7-22841a1b8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    df = feature_selection_tabularize(feature_selection_r2score[model])\n",
    "    df.to_excel(f'{OUTPUT_PATH}/feature_selection_list/feature_selection_r2score_{model}.xlsx', index=False)\n",
    "\n",
    "    df = feature_selection_tabularize(feature_selection_per_diff[model])\n",
    "    df.to_excel(f'{OUTPUT_PATH}/feature_selection_list/feature_selection_per_error_{model}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324a621-6df3-4d1f-ab53-ee4b38863fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r2_top = visualize_highest_score_feature_selection(feature_selection_r2score, f\"{OUTPUT_PATH}/{comparision_model}_5_fold_r2score_backward.png\",    model_name_conversion, only_one_multivariate=only_one_multivariate, legends=True, adj_score=adj_score)\n",
    "visualize_highest_score_feature_selection(feature_selection_per_diff, f\"{OUTPUT_PATH}/{comparision_model}_5_fold_per_error_backward.png\", model_name_conversion, r2_score=False, only_one_multivariate=only_one_multivariate, legends=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72485734-b944-4023-b81d-87c6db8dd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R2 Score and Percent Error on Testing Dataset\n",
    "test_r2_scores, test_per_errors = find_performance_metric(model_names, r2_top)\n",
    "\n",
    "extend_name = ''\n",
    "\n",
    "# Plot the R2 score and Percent Error in the Bar chart\n",
    "visualization_testing_dataset(test_r2_scores,  f\"{OUTPUT_PATH}/{comparision_model}_testing_r2_score{extend_name}.png\",  model_name_conversion, only_one_multivariate, r2_score=True,  adj_score=adj_score, legends=True)\n",
    "visualization_testing_dataset(test_per_errors, f\"{OUTPUT_PATH}/{comparision_model}_testing_per_error{extend_name}.png\", model_name_conversion, only_one_multivariate, r2_score=False, legends=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5aec28-531a-4c49-ac6a-f327f39ccd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
