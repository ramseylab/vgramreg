{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05141b-ef18-425b-a122-8dbcfaffbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.load_dataset import load_dataset\n",
    "from src.load_models  import select_model\n",
    "from src.config import *\n",
    "from src.utils import per_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa02849-c9e9-470a-bcc1-b652dc479792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_create_training_data(train, test, blank_norm=False, remove_outlier=None):\n",
    "\n",
    "    # Remove outlier only from the training dataset\n",
    "    if remove_outlier=='all':\n",
    "        train = train[train['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "        test  = test[test['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "\n",
    "    elif remove_outlier=='train_only':\n",
    "        train = train[train['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "        \n",
    "    train = train.reset_index(drop=True)\n",
    "    test  = test.reset_index(drop=True)\n",
    "    \n",
    "    X_train = train.drop(columns=['file']).copy()\n",
    "    X_test  = test.drop(columns=['file']).copy()\n",
    "\n",
    "    columns       = X_train.columns\n",
    "    \n",
    "    y_train = train['file'].apply(lambda x: int(x.split('_')[-2].replace('cbz','')))\n",
    "    y_test  = test['file'].apply(lambda x: int(x.split('_')[-2].replace('cbz','')))\n",
    "\n",
    "    assert (X_train.index.values == y_train.index.values).all()\n",
    "\n",
    "    scaler  = StandardScaler()\n",
    "\n",
    "    if blank_norm: scaler.fit(X_train[y_train==0])\n",
    "    else: scaler.fit(X_train)\n",
    "\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=columns)\n",
    "    X_test  = pd.DataFrame(scaler.transform(X_test),  columns=columns)\n",
    "\n",
    "    X_train.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n",
    "                        'dS_dV_area':'univariate, area(dS/dV)', 'dS_dV_max_peak':'univariate, max(dS/dV)', 'dS_dV_min_peak':'univariate, min(dS/dV)',\\\n",
    "                    'dS_dV_peak_diff':'univariate, max(dS/dV) - min(dS/dV)', \\\n",
    "                    'peak V':'univariate, V_max(S)', 'dS_dV_max_V':'univariate, V_max(dS/dV)', 'dS_dV_min_V':'univariate, V_min(dS/dV)',\\\n",
    "        }, inplace = True)\n",
    "\n",
    "    X_test.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n",
    "                        'dS_dV_area':'univariate, area(dS/dV)', 'dS_dV_max_peak':'univariate, max(dS/dV)', 'dS_dV_min_peak':'univariate, min(dS/dV)',\\\n",
    "                    'dS_dV_peak_diff':'univariate, max(dS/dV) - min(dS/dV)', \\\n",
    "                    'peak V':'univariate, V_max(S)', 'dS_dV_max_V':'univariate, V_max(dS/dV)', 'dS_dV_min_V':'univariate, V_min(dS/dV)',\\\n",
    "        }, inplace = True)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), scaler\n",
    "\n",
    "def load_dataset_train_test_splitted(filename):\n",
    "    ML1 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_19_ML1/{filename}.xlsx')\n",
    "    ML2 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_22_ML2/{filename}.xlsx')\n",
    "    ML4 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML4/{filename}.xlsx')\n",
    "\n",
    "    return ML1, ML2, ML4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f8a19-869b-4864-b5dc-b652f07e56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ouliners_to_remove = ['2024_02_19_cbz00_15', '2024_02_19_cbz08_40', '2024_02_19_cbz08_37', '2024_02_22_cbz00_01', '2024_02_22_cbz16_21', \n",
    "                     '2024_02_22_cbz08_10', '2024_02_22_cbz08_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403949c-42fd-4983-9e2e-d3ec1ce94de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_propery   = 'augmentation' # noisy, both, and noiseless\n",
    "blank_norm     = False\n",
    "remove_outlier = None\n",
    "\n",
    "ML1_noisy_train, ML2_noisy_train, ML4_noisy_train = load_dataset_train_test_splitted('extracted_features_noisy_training_dataset')\n",
    "ML1_train, ML2_train, ML4_train = load_dataset_train_test_splitted('extracted_features_training_dataset')\n",
    "ML1_test, ML2_test, ML4_test = load_dataset_train_test_splitted('extracted_features_testing_dataset')\n",
    "\n",
    "# Test if training and testing has common dataset\n",
    "assert len(set(ML1_train['file'].values.tolist()) & set(ML1_test['file'].values.tolist()))==0\n",
    "\n",
    "\n",
    "\n",
    "if data_propery=='noisy':\n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_noisy_train, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_noisy_train, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_noisy_train, ML4_test, blank_norm, remove_outlier)\n",
    "    \n",
    "elif data_propery=='augmentation':\n",
    "    ML1_train_combined = pd.concat([ML1_noisy_train, ML1_train])\n",
    "    ML2_train_combined = pd.concat([ML2_noisy_train, ML2_train])\n",
    "    ML4_train_combined = pd.concat([ML4_noisy_train, ML4_train])\n",
    "    \n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_train_combined, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_train_combined, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_train_combined, ML4_test, blank_norm, remove_outlier)\n",
    "    \n",
    "else:\n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_train, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_train, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_train, ML4_test, blank_norm, remove_outlier)\n",
    "\n",
    "\n",
    "X_train = pd.concat([ML1_X_train, ML2_X_train, ML4_X_train], axis=0)\n",
    "y_train = pd.concat([ML1_y_train, ML2_y_train, ML4_y_train], axis=0)\n",
    "\n",
    "indx_shuffle = np.random.permutation(range(len(X_train)))\n",
    "X_train      = X_train.iloc[indx_shuffle].reset_index(drop=True)\n",
    "y_train      = y_train.iloc[indx_shuffle].reset_index(drop=True)\n",
    "\n",
    "X_test  = pd.concat([ML1_X_test,  ML2_X_test,  ML4_X_test], axis=0).reset_index(drop=True)\n",
    "y_test  = pd.concat([ML1_y_test,  ML2_y_test,  ML4_y_test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468877bc-10a2-42d4-8fe4-903529056479",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ab3db-f732-47ec-b6e1-a2a0e8e6b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models = ['Linear', 'KNN', 'SVM', 'RF', 'GP']\n",
    "\n",
    "# Calcualte y_LOD\n",
    "y_LOD = 0.9117010154341669 #calculate_y_LOD(X_testing, y_testing)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4816000-f80c-4bec-b515-8b8d6556fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score_val,  per_diff_val  = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "r2_score_test, per_diff_test = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "\n",
    "mix_par = 0.01\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    model      = select_model(model_name)\n",
    "\n",
    "    teacher_model = clone(model)\n",
    "\n",
    "    hard_labels  = []\n",
    "    soft_labels  = []\n",
    "\n",
    "    X_test_kfold = []\n",
    "\n",
    "    # Create Soft labels from the teacher model\n",
    "    for (train_index, test_index) in kf.split(X_train):\n",
    " \n",
    "        X_training, X_testing = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_training, y_testing = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        teacher_model.fit(X_training[models_features_per[model_name]], y_training)\n",
    "        soft_output_labels = teacher_model.predict(X_testing[models_features_per[model_name]])\n",
    "\n",
    "        X_test_kfold += [X_testing]\n",
    "        soft_labels  += soft_output_labels.tolist()\n",
    "        hard_labels  += y_testing.values.tolist()\n",
    "\n",
    "    y_new_labels   = np.array(soft_labels) * mix_par + np.array(hard_labels) * (1 - mix_par)\n",
    "    X_test_kfold   = pd.concat(X_test_kfold, axis=0)\n",
    "\n",
    "    # Train Student Model\n",
    "    student_model = clone(model)\n",
    "    student_model.fit(X_test_kfold[models_features_per[model_name]], y_new_labels)\n",
    "\n",
    "    y_pred          = student_model.predict(X_test[models_features_per[model_name]])\n",
    "    per_error_final = per_error(y_test, y_pred, y_LOD)\n",
    "\n",
    "    print(f\"Model Name: {model_name}\", f\" | % error: {per_error_final}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570a5ca-104c-4917-9b01-6b8a257ecbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2109bcc-153a-4a30-a8aa-c15910952244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5b04f-5369-42ee-b3c6-9559d4e52b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e958685a-98c0-4f62-ba8d-69bc906372c2",
   "metadata": {},
   "source": [
    "## Outlier with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b8f81-ff44-4cc6-a736-5dc622ae88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outlier(t, st):\n",
    "    st_err = (t - st)\n",
    "\n",
    "    print(\"Median\", np.median(st_err))\n",
    "    MAD    = np.median(st_err - np.median(st_err))\n",
    "    print(np.median(st_err - np.median(st_err)))\n",
    "    sigma  = 1.4826 * MAD\n",
    "    B      = len(t)\n",
    "    alpha  = 1\n",
    "    \n",
    "    epsilon_outlier = sigma * np.sqrt(-2 * np.log(np.sqrt(2 * np.pi) * sigma * alpha / B))\n",
    "\n",
    "    outlier_flag = np.abs(st_err) > epsilon_outlier\n",
    "\n",
    "    return outlier_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9f8b0-5974-405e-a1f4-6e80e3531465",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score_val,  per_diff_val  = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "r2_score_test, per_diff_test = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "\n",
    "mix_par = 0.01\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    model      = select_model(model_name)\n",
    "\n",
    "    teacher_model = clone(model)\n",
    "\n",
    "    hard_labels  = []\n",
    "    soft_labels  = []\n",
    "\n",
    "    X_test_kfold = []\n",
    "\n",
    "    # Create Soft labels from the teacher model\n",
    "    for (train_index, test_index) in kf.split(X_train):\n",
    " \n",
    "        X_training, X_testing = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_training, y_testing = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        teacher_model.fit(X_training[models_features_per[model_name]], y_training)\n",
    "        soft_output_labels = teacher_model.predict(X_testing[models_features_per[model_name]])\n",
    "\n",
    "        outlier_flag = find_outlier(y_testing, soft_output_labels)\n",
    "        if outlier_flag.any():\n",
    "            print(outlier_flag)\n",
    "\n",
    "        break\n",
    "\n",
    "    break\n",
    "\n",
    "        # X_test_kfold += [X_testing]\n",
    "        # soft_labels  += soft_output_labels.tolist()\n",
    "        # hard_labels  += y_testing.values.tolist()\n",
    "\n",
    "    # y_new_labels   = np.array(soft_labels) * mix_par + np.array(hard_labels) * (1 - mix_par)\n",
    "    # X_test_kfold   = pd.concat(X_test_kfold, axis=0)\n",
    "\n",
    "    # # Train Student Model\n",
    "    # student_model = clone(model)\n",
    "    # student_model.fit(X_test_kfold[models_features_per[model_name]], y_new_labels)\n",
    "\n",
    "    # y_pred          = student_model.predict(X_test[models_features_per[model_name]])\n",
    "    # per_error_final = per_error(y_test, y_pred, y_LOD)\n",
    "\n",
    "    print(f\"Model Name: {model_name}\", f\" | % error: {per_error_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a6a58-af5b-42a4-8bc7-571372933588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
