{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd88226-4ab6-4d28-88b6-821d3bd6c147",
   "metadata": {},
   "source": [
    "In this experiment, I train the models with noisy data and test the performance on test data.<br>\n",
    "- If the performace of test dataset is comparable with the performance trained on noise free dataset then the data augmentation is helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6b81df-9557-40eb-9fa0-39690e721106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pickle import load, dump\n",
    "\n",
    "from src.load_dataset import load_dataset\n",
    "from src.utils import tsen_pca_viz, verify_batch_label_dist, calculate_r2_score, calculate_per_diff, per_error, find_adj_score, combine_all_batches, split_batches_back, perform_combat_normalization\n",
    "from src.load_models import select_model\n",
    "from src.graph_visualization import visualization_testing_dataset\n",
    "from src.config import *\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef56611f-f276-4720-8f2f-62ecdd04f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_outlier_removal(o_removal = 'iforest'):\n",
    "    if o_removal=='iforest': return IForest(contamination=0.03,n_estimators=100,\n",
    "                                                 random_state=0) \n",
    "    elif o_removal=='lof':   return LOF(contamination=0.03, n_neighbors=5) \n",
    "\n",
    "def remove_outlier_box_plot():\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def find_outliers_in_data(all_combined, labels, outlier_removal_algo, outlier_threshold=0.7):\n",
    "    ouliers_name = []\n",
    "\n",
    "    for label in labels:\n",
    "        \n",
    "        data  = all_combined[all_combined['y']==label].copy()\n",
    "        tx    = data.drop(columns=['file', 'y'])\n",
    "\n",
    "        if outlier_removal_algo != 'IQR':\n",
    "            o_removal = select_outlier_removal(outlier_removal_algo)\n",
    "            o_removal.fit(tx)\n",
    "            predicted = pd.Series(o_removal.predict(tx),index=tx.index)\n",
    "            \n",
    "            outliers = predicted[predicted > outlier_threshold] \n",
    "            outliers = data.loc[outliers.index] \n",
    "            ouliers_name += outliers['file'].map(lambda x: x.split('/')[-1].replace('.txt', '')).to_list()\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    return ouliers_name\n",
    "    \n",
    "def normalize_create_training_data(train, test, blank_norm=False, remove_outlier=None):\n",
    "\n",
    "    # Remove outlier only from the training dataset\n",
    "    if remove_outlier=='all':\n",
    "        \n",
    "        train = train[train['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "        test  = test[test['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "\n",
    "    elif remove_outlier=='train_only':\n",
    "        train = train[train['file'].apply(lambda x: False if (x.split('/')[-1].replace('.txt', '') in ouliners_to_remove) else True)]\n",
    "        \n",
    "    train = train.reset_index(drop=True)\n",
    "    test  = test.reset_index(drop=True)\n",
    "    \n",
    "    X_train = train.drop(columns=['file']).copy()\n",
    "    X_test  = test.drop(columns=['file']).copy()\n",
    "\n",
    "    columns       = X_train.columns\n",
    "    \n",
    "    y_train = train['file'].apply(lambda x: int(x.split('_')[-2].replace('cbz','')))\n",
    "    y_test  = test['file'].apply(lambda x: int(x.split('_')[-2].replace('cbz','')))\n",
    "\n",
    "    assert (X_train.index.values == y_train.index.values).all()\n",
    "\n",
    "    scaler  = StandardScaler()\n",
    "\n",
    "    if blank_norm: scaler.fit(X_train[y_train==0])\n",
    "    else: scaler.fit(X_train)\n",
    "\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=columns)\n",
    "    X_test  = pd.DataFrame(scaler.transform(X_test),  columns=columns)\n",
    "\n",
    "    X_train.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n",
    "                        'dS_dV_area':'univariate, area(dS/dV)', 'dS_dV_max_peak':'univariate, max(dS/dV)', 'dS_dV_min_peak':'univariate, min(dS/dV)',\\\n",
    "                    'dS_dV_peak_diff':'univariate, max(dS/dV) - min(dS/dV)', \\\n",
    "                    'peak V':'univariate, V_max(S)', 'dS_dV_max_V':'univariate, V_max(dS/dV)', 'dS_dV_min_V':'univariate, V_min(dS/dV)',\\\n",
    "        }, inplace = True)\n",
    "\n",
    "    X_test.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n",
    "                        'dS_dV_area':'univariate, area(dS/dV)', 'dS_dV_max_peak':'univariate, max(dS/dV)', 'dS_dV_min_peak':'univariate, min(dS/dV)',\\\n",
    "                    'dS_dV_peak_diff':'univariate, max(dS/dV) - min(dS/dV)', \\\n",
    "                    'peak V':'univariate, V_max(S)', 'dS_dV_max_V':'univariate, V_max(dS/dV)', 'dS_dV_min_V':'univariate, V_min(dS/dV)',\\\n",
    "        }, inplace = True)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), scaler\n",
    "\n",
    "def load_dataset_train_test_splitted(filename):\n",
    "    ML1 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_19_ML1/{filename}.xlsx')\n",
    "    ML2 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML1_ML2/2024_02_22_ML2/{filename}.xlsx')\n",
    "    ML4 = pd.read_excel(f'/Users/sangam/Desktop/Epilepsey/Code/vgramreg/dataset/ML4/{filename}.xlsx')\n",
    "\n",
    "    return ML1, ML2, ML4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09aeeb7f-190f-4838-b465-d4ff7526177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouliners_to_remove = ['2024_03_08_cbz16_36', '2024_03_08_cbz16_15']\n",
    "\n",
    "# ouliners_to_remove = ['2024_02_19_cbz08_43',\n",
    "#                       '2024_02_19_cbz08_37',\n",
    "#                       '2024_02_22_cbz08_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86fd754a-af21-45fa-8caf-aa0767167e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML1_noisy_train, ML2_noisy_train, ML4_noisy_train = load_dataset_train_test_splitted('feature_extraction_vwidth_0.15_training_noisy')\n",
    "ML1_train, ML2_train, ML4_train = load_dataset_train_test_splitted('feature_extraction_vwidth_0.15_training')\n",
    "ML1_test, ML2_test, ML4_test = load_dataset_train_test_splitted('feature_extraction_vwidth_0.15_testing')\n",
    "\n",
    "# Test if training and testing has common dataset\n",
    "assert len(set(ML1_train['file'].values.tolist()) & set(ML1_test['file'].values.tolist()))==0\n",
    "assert len(set(ML1_noisy_train['file'].values.tolist()) & set(ML1_test['file'].values.tolist()))==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87baa872-3c6e-4dd4-8741-1374a7cb2159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024_02_22_cbz00_31',\n",
       " '2024_02_22_cbz00_18',\n",
       " '2024_03_08_cbz00_38',\n",
       " '2024_02_19_cbz08_37',\n",
       " '2024_02_22_cbz08_01',\n",
       " '2024_03_08_cbz08_04',\n",
       " '2024_02_19_cbz16_28',\n",
       " '2024_02_19_cbz16_01',\n",
       " '2024_03_08_cbz16_15']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_combined_data  = pd.concat([ML1_train, ML2_train, ML4_train], axis=0).reset_index(drop=True) \n",
    "all_combined_data['y'] = all_combined_data['file'].apply(lambda x: int(x.split('_')[-2].replace('cbz','')))                                                                           \n",
    "ouliners_to_remove = find_outliers_in_data(all_combined_data, all_combined_data['y'].unique(), outlier_removal_algo='iforest')\n",
    "\n",
    "ouliners_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32c931e-777e-498d-8448-7d0626c9da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_propery   = 'noiseless' # noisy, augmentation, and noiseless\n",
    "blank_norm     = False\n",
    "remove_outlier = ''   #None, train_only, all\n",
    "\n",
    "if data_propery=='noisy':\n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_noisy_train, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_noisy_train, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_noisy_train, ML4_test, blank_norm, remove_outlier)\n",
    "    \n",
    "elif data_propery=='augmentation':\n",
    "    ML1_train_combined = pd.concat([ML1_noisy_train, ML1_train])\n",
    "    ML2_train_combined = pd.concat([ML2_noisy_train, ML2_train])\n",
    "    ML4_train_combined = pd.concat([ML4_noisy_train, ML4_train])\n",
    "    \n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_train_combined, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_train_combined, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_train_combined, ML4_test, blank_norm, remove_outlier)\n",
    "    \n",
    "else:\n",
    "    (ML1_X_train, ML1_X_test, ML1_y_train, ML1_y_test), ML1_scalar = normalize_create_training_data(ML1_train, ML1_test, blank_norm, remove_outlier)\n",
    "    (ML2_X_train, ML2_X_test, ML2_y_train, ML2_y_test), ML2_scalar = normalize_create_training_data(ML2_train, ML2_test, blank_norm, remove_outlier)\n",
    "    (ML4_X_train, ML4_X_test, ML4_y_train, ML4_y_test), ML4_scalar = normalize_create_training_data(ML4_train, ML4_test, blank_norm, remove_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1c8d0f-a7ba-416f-aa42-15bd4300ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([ML1_X_train, ML2_X_train, ML4_X_train], axis=0)\n",
    "y_train = pd.concat([ML1_y_train, ML2_y_train, ML4_y_train], axis=0)\n",
    "\n",
    "indx_shuffle = np.random.permutation(range(len(X_train)))\n",
    "X_train      = X_train.iloc[indx_shuffle]\n",
    "y_train      = y_train.iloc[indx_shuffle]\n",
    "\n",
    "X_test  = pd.concat([ML1_X_test,  ML2_X_test,  ML4_X_test], axis=0)\n",
    "y_test  = pd.concat([ML1_y_test,  ML2_y_test,  ML4_y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dec587a-7f16-48e2-9a99-677eeb35734a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ML2_X_train.index.values == ML2_y_train.index.values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbe3dd0-b5ed-4b50-9460-1163d0a97334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((216, 13), (145, 13))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af98296-03cb-45ca-891e-63ce1705a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models = ['Linear', 'KNN', 'SVM', 'RF', 'GP']\n",
    "\n",
    "# Calcualte y_LOD\n",
    "y_LOD = 0.9117010154341669 #calculate_y_LOD(X_testing, y_testing)\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "r2_score_val,  per_diff_val  = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "r2_score_test, per_diff_test = {'Models':[], 'Scores':[]}, {'Models':[], 'Scores':[]}\n",
    "\n",
    "for model_name in models:\n",
    "    model      = select_model(model_name)\n",
    "\n",
    "    val_r2     = calculate_r2_score(model, X_train[models_features_r2[model_name]],  y_train, kf)\n",
    "    val_per    = calculate_per_diff(model, X_train[models_features_per[model_name]], y_train, kf, y_LOD)\n",
    "\n",
    "    r2_score_val['Scores'].append(val_r2)\n",
    "    per_diff_val['Scores'].append(val_per)\n",
    "\n",
    "    model_r2  = clone(model)\n",
    "    model_r2.fit(X_train[models_features_r2[model_name]], y_train)\n",
    "    y_pred_r2 = model_r2.predict(X_test[models_features_r2[model_name]])\n",
    "\n",
    "    r2_test_score = r2_score(y_test, y_pred_r2)\n",
    "    adj_r2_test   = find_adj_score(len(y_pred_r2), len(models_features_r2[model_name]), r2_test_score)\n",
    "    \n",
    "    r2_score_test['Scores'].append((r2_test_score, adj_r2_test))\n",
    "\n",
    "    model_per_diff = clone(model)\n",
    "    model_per_diff.fit(X_train[models_features_per[model_name]], y_train)\n",
    "    y_pred_per_diff = model_per_diff.predict(X_test[models_features_per[model_name]])\n",
    "    \n",
    "    per_diff_test['Scores'].append(per_error(y_test, y_pred_per_diff, y_LOD))\n",
    "\n",
    "    r2_score_val['Models'].append(model_name)\n",
    "    per_diff_val['Models'].append(model_name) \n",
    "    r2_score_test['Models'].append(model_name)\n",
    "    per_diff_test['Models'].append(model_name)\n",
    "\n",
    "    save_model_name = f'../models/Data_{data_propery}_outlier_remove_{remove_outlier}_vwidth_0.15'\n",
    "    os.makedirs(save_model_name, exist_ok=True)\n",
    "\n",
    "    with open(f'{save_model_name}/{model_name}.pickle', 'wb') as f:\n",
    "        dump(model_per_diff, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ddc848d-13be-498e-bc2b-d9d1c331e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/Data_noiseless_outlier_remove__vwidth_0.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.986712074857145"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(save_model_name)\n",
    "model_name = 'GP'\n",
    "with open(f'{save_model_name}/{model_name}.pickle', 'rb') as f:\n",
    "    model = load(f)\n",
    "y_pred = model.predict(X_test[models_features_per[model_name]])\n",
    "\n",
    "per_error(y_test, y_pred, y_LOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f663dec6-40fc-4142-b22c-a4f603c2e583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     16\n",
       "1      0\n",
       "2      8\n",
       "3     16\n",
       "4     16\n",
       "      ..\n",
       "41    16\n",
       "42    16\n",
       "43     8\n",
       "44     0\n",
       "45     8\n",
       "Name: file, Length: 145, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd0cfc4-dc2e-42a0-b7b8-09fb0fd2ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savedir   = f'../results/Noisy_Training_Dataset/data_property_{data_propery}_blank_norm_{blank_norm}_outlier_remove_{remove_outlier}_vwidth_0.15'\n",
    "adj_score = False\n",
    "\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "visualization_testing_dataset(r2_score_val,  f'{savedir}/r2_score_val.png',   model_name_conversion, only_one_multivariate=False, adj_score=adj_score, legends=True)\n",
    "visualization_testing_dataset(per_diff_val, f'{savedir}/per_error_val.png', model_name_conversion, only_one_multivariate=False, r2_score=False, adj_score=False, legends=True)\n",
    "\n",
    "visualization_testing_dataset(r2_score_test,  f'{savedir}/r2_score_test.png',   model_name_conversion, only_one_multivariate=False, adj_score=adj_score, legends=True)\n",
    "visualization_testing_dataset(per_diff_test, f'{savedir}/per_error_test.png', model_name_conversion, only_one_multivariate=False, r2_score=False, adj_score=False, legends=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c15cf1-f6a2-4b65-89a4-56bf060e4918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95d86d-3348-4818-af93-b04c4164819e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03c2de-3bc6-4fa2-bea3-6e8dee5e9c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d86195d-7de4-49f3-8be7-b47dffde8d0d",
   "metadata": {},
   "source": [
    "## Outlier plot with box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c3e17-86f1-4dad-8a43-921fd8f31264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "feature = 'PH'\n",
    "\n",
    "# Find outliers \n",
    "for i in all_combined_data['y'].unique():\n",
    "    Q1  = np.percentile(all_combined_data[all_combined_data['y']==i][feature], q=25)\n",
    "    Q3  = np.percentile(all_combined_data[all_combined_data['y']==i][feature], q=75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5*IQR\n",
    "    upper_bound = Q3 + 1.5*IQR\n",
    "\n",
    "    above_upper_bound = all_combined_data[[feature, 'file']][(all_combined_data[feature]>upper_bound) & (all_combined_data['y']==i)]\n",
    "    above_lower_bound = all_combined_data[[feature, 'file']][(all_combined_data[feature]<lower_bound) & (all_combined_data['y']==i)]\n",
    "\n",
    "    print(f\"Con:{i}\", \"Above_upper_bound:\", above_upper_bound['file'].values, \" | \",\"Lower_bound\",  above_lower_bound['file'].values)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.boxplot(all_combined_data, x='y', y=feature)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f11206-529c-49c2-b23e-5998e39d6cc9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe3d4d-821c-455d-bb68-2ab30d4c535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2  = select_model('GP')\n",
    "model_r2.fit(X_train[models_features_r2[model_name]], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866a6f0-7643-4596-8907-aac24d64e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename  = '2024_02_22_cbz00_01'\n",
    "test_data = ML2_test[ML2_test['file'].apply(lambda x: True if filename in x else False)]\n",
    "test_data.drop(columns='file', inplace=True)\n",
    "columns   = test_data.columns\n",
    "test_data = pd.DataFrame(ML2_scalar.transform(test_data), columns=columns)\n",
    "\n",
    "test_data.rename(columns={\"PH\": 'univariate, max(S)', 'signal_std':'univariate, std(S)', 'signal_mean':'univariate, mean(S)', 'peak area':'univariate, area(S)', \\\n",
    "                        'dS_dV_area':'univariate, area(dS/dV)', 'dS_dV_max_peak':'univariate, max(dS/dV)', 'dS_dV_min_peak':'univariate, min(dS/dV)',\\\n",
    "                    'dS_dV_peak_diff':'univariate, max(dS/dV) - min(dS/dV)', \\\n",
    "                    'peak V':'univariate, V_max(S)', 'dS_dV_max_V':'univariate, V_max(dS/dV)', 'dS_dV_min_V':'univariate, V_min(dS/dV)',\\\n",
    "        }, inplace = True)\n",
    "\n",
    "test_data.shape\n",
    "model_r2.predict(test_data[models_features_r2[model_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff51b0c-8ea2-42e7-a472-3c221243bce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
